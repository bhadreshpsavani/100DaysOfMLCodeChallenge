Day1: 21st March 2019

I learn that we can make OCR based model for single line of text using Keras and TensorFlow.

*Conectionist Temporal Classification (CTC) Loss function : Applied to identified word by removing
    words and white spaces

We can make OCR based model for entire text consist of multiple line, for that we need to do some Preprossessing

DIA(Document Image Analysis): 
                            1. Image Segmentation
                            2. word Extraction
                            3. Text Recongnition using OCR 
                            4. Error reduction of words using Language model or Dictionary APIs

3. Text Recongnition using OCR :
    2 methods:
        1. LSTM
        2. Uniform Segmentation

findContours function of opencv

Line Normalization using 'ocrolib':
    'https://github.com/tmbdev/ocropy/blob/master/doc/line-normalization.ipynb'

*To avoid network overfitting we stopped the training process several times and 
continued training the network with the new dataset

Resources:
'https://dzone.com/articles/using-ocr-for-receipt-recognition';

Day2: 22 march 2019

started Watching videos related to using python for AI.

wanted to learn libraries like 1. numpy: for array manipulation
                               2. Pandas: data Manipulation and Analysis
                               3. scikit: data Visulization
                               4. python: facebook lib for deep learning
                               
Day3: 24-03-2019

Pandas: 
- Dataframe is a heterogeneous object and hence are able to store different datatypes together

Day6: 27-03-2019

supervised learning: the user provides the algorithm with pairs of inputs and desired outputs,
and the algorithm finds a way to produce the desired output given an input

unsupervised learning:
only the input data is known, and no known output data is given to the algorithm

feature extraction or feature engineering: building a good representation of your data

k-Nearest Neighbors Algorithm:

- To make a prediction for a new data point, the algorithm finds the point in the training set that is closest to the new point 
- The k in k-nearest neighbors signifies that instead of using only the closest neighbor to the new data point, we can consider any fixed number k of neighbors in the training.

*  scikit-learn always expects two-dimensional arrays for the data. 

-  The fit, predict, and score methods are the common interface to supervised models in scikit-learn

Day7: 28 Mar 2019

- Datasets that are included in scikit-learn are usually stored as Bunch objects, which contain some information about the dataset
as well as the actual data. All you need to know about Bunch objects is that they behave like dictionaries, with the added benefit that you can access values using a dot

-feature engineering on boston dataset we can get more feature by doing product of normal features.

-  The test set accuracy for using a single neighbor is lower than when using more neighbors, indicating that using the single nearest neighbor leads to a model that is too complex. On the other hand, when considering 10 neighbors, the model is too simple and performance is even worse. The best performance is somewhere in the middle, using around six neighbors

* while the nearest k-neighbors algorithm is easy to understand, it is not often used
in practice, due to prediction being slow and its inability to handle many features.

You might notice the strange-looking trailing underscore at the end of coef_ and intercept_. scikit-learn always stores anything
that is derived from the training data in attributes that end with a trailing underscore. That is to separate them from parameters that
are set by the user

*****************Regression Problems*****************

1. Linear Regression
2. Ridge Regression (uses L2 Regularization)
3. Lasso Regression (uses L1 Regularization)

Linear Regression: 

If more data is added, it becomes harder for a model to overfit, or memorize the data. 

** We also want the magnitude of coefficients to be as small as possible; in other words, all entries of w should be close to
zero. Intuitively, this means each feature should have as little effect on the outcome as possible (which translates to having a small slope), while still predicting well. This constraint is an example of what is called regularization. 

-"Regularization means explicitly restricting a model to avoid overfitting". 

With linear regression, we were overfitting our data. Ridge is a more restricted model, so we are less likely to overfit

we are only interested in generalization performance, we should choose the Ridge model over the LinearRegression model.

* plots that show model performance as a function of dataset size are called learning curves

The consequence of L1 regularization is that when using the lasso, some coefficients are exactly zero
