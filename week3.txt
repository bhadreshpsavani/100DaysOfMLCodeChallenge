Day1: 29 mar 2019

Binary Classifier for multiclass problem,

To make a prediction, all binary classifiers are run on a test point. The classifier that has the highest score on its
single class “wins,” and this class label is returned as the prediction.

Decision Tree:

Random Forest: uses two algorithms to make each tree different from each other
        1. Bootstrap sampling
        2. feature selection
        
Critical feature in random forest:

  max_feature parameter: a high max_features means that the trees in the random forest will be quite similar, and they will be
                able to fit the data easily, using the most distinctive features.
                
                         A low max_features means that the trees in the random forest will be quite different, and that each tree
                might need to be very deep in order to fit the data well.

n_estimators: Higher is better

it’s a good rule of thumb to use the default values:  max_features=sqrt(n_features)  for classification and 
                                                      max_features=log2(n_features) for regression

To make a prediction using the random forest, the algorithm first makes a prediction for every tree in the forest. For regression, we can average these results to get our final prediction. For classification, a “soft voting” strategy is used. This means each algorithm makes a  “soft” prediction, providing a probability for each possible output

Gradient boosted regression trees (gradient boosting machines): 1. learning_rate
                                                                2. n_estimartors
                                                    
                                                    
Support Vector Machines: 

Day2: 30 Mar 2019

Deep Learning:

MultiLayerPerceptons(MLP): After computing a weighted sum for each hidden unit, a nonlinear function is applied to the result—usually the rectifying nonlinearity (also known as rectified linear unit or relu) or the tangens hyperbolicus (tanh)

* The relu cuts off values below zero,
* tanh saturates to –1 for low input values and +1 for high input values.

        it have regularization parameter alpha if it is higher then model is more general and if low then model is complex or overfitting
        
- Neural networks—particularly the large and powerful ones—often take a long time to train. 
- They also require careful preprocessing of the data.
- Similarly to SVMs, they work best with “homogeneous” data, where all the features have similar meanings. 
- For data that has very different kinds of features, tree-based models might work better. 

* fit Resets a Model: 
An important property of scikit-learn models is that calling fit will always reset everything a model previously learned. So if you
build a model on one dataset, and then call fit again on a different dataset, the model will “forget” everything it learned from the first dataset.

There is also the question of how to learn the model, or the algorithm that is used for learning the parameters, which is set using the algorithm parameter: Default Adam, l-bfgs, sgd

scikit-learn has ability of classifiers to provide uncertainty estimates of predictions.

scikit-learn that can be used to obtain uncertainty estimates from classifiers: decision_function and predict_proba

predict_proba and decision_function always have shape (n_samples, n_classes)—apart from decision_function in the special binary case. In the binary case, decision_function only has one column, corresponding to the “positive” class classes_[1]

************************Which Algorithm Should We use***********************

Nearest neighbors: For small datasets, good as a baseline, easy to explain.

Linear models: Go-to as a first algorithm to try, good for very large datasets, good for very highdimensional data.

Naive Bayes: Only for classification. Even faster than linear models, good for very large datasets and high-dimensional data. Often less accurate than linear models.

Decision trees: Very fast, don’t need scaling of the data, can be visualized and easily explained.

Random forests: Nearly always perform better than a single decision tree, very robust and powerful. Don’t need scaling of data. Not good for very high-dimensional sparse data.

Gradient boosted decision trees: Often slightly more accurate than random forests. Slower to train but faster to predict than random forests, and smaller in memory. Need more parameter tun‐ing than random forests.

**********************Unsupervised Learning**********************************

Unsupervised transformations of a dataset are algorithms that create a new representation of the data which might be easier for humans or other machine learning algorithms to understand compared to the original representation of the data.

Different Scaler for Preprocessing:
        1. StandardScaler,
        2. RobustScaler,
        3. MinMaxScaler,
        4. Normalizer
   
Other Preprocessing Algorithms:
        1. Principal Component Analysis (PCA),
        2. Non-negative Matrix Factorization (NMF),
        3. t-SNE.

Principal component analysis is a method that rotates the dataset in a way such that the rotated features are statistically uncorrelated. Using PCA, we can capture the main interactions and get a slightly more complete picture. We can find the first two principal components, and visualize the data in this new two-dimensional space with a single scatter plot. The principal components correspond to directions in the original data, so they are combinations of the original features.

non-negative matrix factorization (NMF), which is commonly used for feature extraction, 

t-SNE, which is commonly used for visualization using two-dimensional scatter plots.
----------------------------------------------------------------------------------------------------------------------------------------

Day3: 2 April 2019

-------------------------------------------------Model Evaluation and Improvement------------------------------------------------------

Cross-validation is a statistical method of evaluating generalization performance that is more stable and thorough than using a split into a training and a test set

"It is important to keep in mind that cross-validation is not a way to build a model that can be applied to new data. Cross-validation
does not return a model. When calling cross_val_score, multiple models are built internally, but the purpose of cross-validation is
only to evaluate how well a given algorithm will generalize when trained on a specific dataset."

k-fold cross-validation:
The main disadvantage of cross-validation is increased computational cost

Stratified k-Fold Cross-Validation:In stratified cross-validation, we split the data such that the proportions between classes are the same in each fold as they are in the whole dataset

It is usually a good idea to use stratified k-fold cross-validation instead of k-fold cross-validation to evaluate a classifier, because it results in more reliable estimates of generalization performance. 

regression, scikit-learn uses the standard k-fold cross-validation by default.
