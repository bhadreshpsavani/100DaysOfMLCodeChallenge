Day1: 4th May 2019 

						(Deep Learning Basics)

Linear regression helps predict values on a continuous spectrum, like predicting what the price of a house will be.

Classification problems are important for self-driving cars. Self-driving cars might need to classify 
whether an object crossing the road is a car, pedestrian, and a bicycle. 
Or they might need to identify which type of traffic sign is coming up, or what a stop light is indicating.

Data, like test scores and grades, are fed into a network of interconnected nodes. These individual nodes are called perceptrons, 
or artificial neurons, and they are the basic unit of a neural network. Each one looks at input data and decides how to categorize 
that data
	
When input comes into a perceptron, it gets multiplied by a weight value that is assigned to this particular 
	
Each input to a perceptron has an associated weight that represents its importance. These weights are determined during the learning 
process of a neural network, called training.
	
The activation function that decides the actual output, we often refer to the outputs of a layer as its "activations".

Then the neural network starts to learn! Initially, the weights and bias are assigned a random value, and then they are updated using 
a learning algorithm like gradient descent.

Maximum Likelihood: Maximize the total probability

Cross Entropy: sum of negative of logarithm of probabilities. This Negative of logarithm indicates error.
good model - low cross entropy - high product of probability
bad model - high cross entropy - low product of probability
Goal - Maximizing probalities - Minimizing crossentropy

Percepton algorithm and Gradient Decenet Algorithm

Day2: 5th May 2019

						(TensorFlow)

The tensor returned by tf.constant() is called a constant tensor, because the value of the tensor never changes.

Code:
	x = tf.placeholder(tf.string)
	y = tf.placeholder(tf.int32)
	z = tf.placeholder(tf.float32)

	with tf.Session() as sess:
	    output = sess.run(x, feed_dict={x: 'Test String', y: 123, z: 45.67})

Here, placeholder is like a variable and feed_dict is used to assign value to the variable

tf.cast(tf.constant(2.0) : casting float to integer value

tf.add(), tf.subtract(), tf.multiply(), and tf.divide()

Logistic Classifier = Linear Classifier
Scores in the terms of logistic Regression are also called as logists

	n_features = 120
	n_labels = 5
	weights = tf.Variable(tf.truncated_normal((n_features, n_labels)))
The tf.truncated_normal() function returns a tensor with random values from a normal distribution whose magnitude is no more than 2 standard deviations from the mean.

tf.Variable() create tensor variable

	init = tf.global_variables_initializer()
	with tf.Session() as sess:
	    sess.run(init)

The tf.global_variables_initializer() call returns an operation that will initialize all TensorFlow variables from the graph.

The tf.zeros() function returns a tensor with all zeros.

tf.matmul() : For Matrics Multiplication

We can take advantage of an operation called broadcasting used in TensorFlow and Numpy. This operation allows arrays of different dimension to be added or multiplied with each other. For example:

import numpy as np
t = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9], [10, 11, 12]])
u = np.array([1, 2, 3])
print(t + u)
The code above will print...

[[ 2  4  6]
 [ 5  7  9]
 [ 8 10 12]
 [11 13 15]]
This is because u is the same dimension as the last dimension in t.

tf.nn.softmax(tensor)

Multinominal Logistic Classification

