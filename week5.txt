Day1: 4th May 2019 

						(Deep Learning Basics)

Linear regression helps predict values on a continuous spectrum, like predicting what the price of a house will be.

Classification problems are important for self-driving cars. Self-driving cars might need to classify 
whether an object crossing the road is a car, pedestrian, and a bicycle. 
Or they might need to identify which type of traffic sign is coming up, or what a stop light is indicating.

Data, like test scores and grades, are fed into a network of interconnected nodes. These individual nodes are called perceptrons, 
or artificial neurons, and they are the basic unit of a neural network. Each one looks at input data and decides how to categorize 
that data
	
When input comes into a perceptron, it gets multiplied by a weight value that is assigned to this particular 
	
Each input to a perceptron has an associated weight that represents its importance. These weights are determined during the learning 
process of a neural network, called training.
	
The activation function that decides the actual output, we often refer to the outputs of a layer as its "activations".

Then the neural network starts to learn! Initially, the weights and bias are assigned a random value, and then they are updated using 
a learning algorithm like gradient descent.

Maximum Likelihood: Maximize the total probability

Cross Entropy: sum of negative of logarithm of probabilities. This Negative of logarithm indicates error.
good model - low cross entropy - high product of probability
bad model - high cross entropy - low product of probability
Goal - Maximizing probalities - Minimizing crossentropy

Percepton algorithm and Gradient Decenet Algorithm

Day2: 5th May 2019

						(TensorFlow)

The tensor returned by tf.constant() is called a constant tensor, because the value of the tensor never changes.

Code:
	x = tf.placeholder(tf.string)
	y = tf.placeholder(tf.int32)
	z = tf.placeholder(tf.float32)

	with tf.Session() as sess:
	    output = sess.run(x, feed_dict={x: 'Test String', y: 123, z: 45.67})

Here, placeholder is like a variable and feed_dict is used to assign value to the variable

tf.cast(tf.constant(2.0) : casting float to integer value

tf.add(), tf.subtract(), tf.multiply(), and tf.divide()

Logistic Classifier = Linear Classifier
Scores in the terms of logistic Regression are also called as logists

	n_features = 120
	n_labels = 5
	weights = tf.Variable(tf.truncated_normal((n_features, n_labels)))
The tf.truncated_normal() function returns a tensor with random values from a normal distribution whose magnitude is no more than 2 standard deviations from the mean.

tf.Variable() create tensor variable

	init = tf.global_variables_initializer()
	with tf.Session() as sess:
	    sess.run(init)

The tf.global_variables_initializer() call returns an operation that will initialize all TensorFlow variables from the graph.

The tf.zeros() function returns a tensor with all zeros.

tf.matmul() : For Matrics Multiplication

We can take advantage of an operation called broadcasting used in TensorFlow and Numpy. This operation allows arrays of different dimension to be added or multiplied with each other. For example:

import numpy as np
t = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9], [10, 11, 12]])
u = np.array([1, 2, 3])
print(t + u)
The code above will print...

[[ 2  4  6]
 [ 5  7  9]
 [ 8 10 12]
 [11 13 15]]
This is because u is the same dimension as the last dimension in t.

tf.nn.softmax(tensor)

Multinominal Logistic Classification

Day3: 7th May 2019

 # Turn labels into numbers and apply One-Hot Encoding
    from sklearn.preprocessing import LabelBinarizer
    encoder = LabelBinarizer()
    encoder.fit(train_labels)
    train_labels = encoder.transform(train_labels)
    test_labels = encoder.transform(test_labels)
    
# TODO: Set the features and labels tensors
features = tf.placeholder(tf.float32, [None, features_count])
labels = tf.placeholder(tf.float32, [None, labels_count]) 

# TODO: Set the weights and biases tensors
weights = tf.Variable(tf.random_normal([features_count, labels_count]))
bias = tf.Variable(tf.random_normal([labels_count]))

Day4: 8th May 2018

The tf.nn.dropout() function takes in two parameters:

	hidden_layer: the tensor to which you would like to apply dropout
	keep_prob: the probability of keeping (i.e. not dropping) any given unit
	
During training, a good starting value for keep_prob is 0.5.

During testing and Validation, use a keep_prob value of 1.0 to keep all units and maximize the power of the model.

Day5: 10th May 2019

In python, In for loop if we want to iterate list with keeping track of index we use: enumerate() in for loop

Day6: 11th May 2019

Embedding Layer: 
The Embedding layer takes as input a 2D tensor of integers, of shape (samples, sequence_length),
This layer returns a 3D floating-point tensor of shape (samples, sequence_length, embedding_dimensionality)
ex. model.add(Embedding(10000, 8, input_length=maxlen)) 
here, number of most commen words: 10000, embedding_dimensionality: 8
Note: This layer don't consider inter word relationship.

Pretrained WordEmbedding:
Word2vec dimensions capture specific semantic properties, such as gender
GloVe

Day7: 12th May 2019

Keras has image preprocessing module in which we use ImageDataGenerator class to get images 
we can get it in many way like,
datagen.flow(x_train, y_train, batch_size=32) # if we have data in the form of x_train, y_train
train_datagen.flow_from_directory('data/train', target_size=(150, 150), batch_size=32, class_mode='binary') #when data is stored in 														     specific lib

